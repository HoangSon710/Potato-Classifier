# -*- coding: utf-8 -*-
"""Predicter -  Potato diaseas Classification .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NQQAWPtA91mJTgJ44R8Gy65VvncoGrUk
"""

import tensorflow as tf
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt
import numpy as np
from tensorflow import keras
from tensorflow.math import reduce_prod
import os

from google.colab import drive
drive.mount('/content/drive')

New_model = tf.keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/my_model.h5')
New_model.summary()

New_model.compile(
    optimizer='adam',
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits =False),
    metrics = ['accuracy']
)

IMAGE_SIZE = 256
BATCH_SIZE = 32
CHANNELS = 3
EPOCHS = 50

data_set = tf.keras.preprocessing.image_dataset_from_directory('/content/drive/MyDrive/Project/Dataset Potatoes/PlantVillage', 
                                                               shuffle = True, 
                                                               seed = 1,
                                                               image_size = (IMAGE_SIZE,IMAGE_SIZE), 
                                                               batch_size = BATCH_SIZE)

class_names = data_set.class_names
class_names

def get_dataset_partition_tf (ds, train_split = 0.8, val_split = 0.1, test_split = 0.1, shuffle = True, shuffle_size = 10000):
    ds_size = len(ds)
    if shuffle:
        ds = ds.shuffle(shuffle_size, seed = 12)
    
    train_size = int(train_split*ds_size)
    val_size = int(val_split*ds_size)
    
    train_ds = ds.take(train_size)
    
    val_ds = ds.skip(train_size).take(val_size)
    test_ds = ds.skip(train_size).skip(val_size)
    
    return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds = get_dataset_partition_tf (data_set)

AUTOTUNE = tf.data.experimental.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)

Input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)
n_classes = 3

for images_batch, labels_batch in test_ds.take(1):
  first_image = images_batch[0].numpy().astype('uint8')
  first_label = labels_batch[0].numpy()

  print('first image to predict')
  plt.imshow(first_image)
  print("first image's actual label: ", class_names[first_label])

  batch_prediction = New_model.predict(images_batch)
  print ("first image's predicted label: ", class_names[np.argmax(batch_prediction[0])])

def predict (model, img):
  img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())
  img_array = tf.expand_dims(img_array, 0) #creat a batch

  predictions = model.predict(img_array)

  predicted_class = class_names[np.argmax(batch_prediction[0])]
  confidence = round (100* (np.max(predictions[0])),2)
  return predicted_class, confidence

plt.figure(figsize = (15,15))
for images, labels in test_ds.take(1):
  for i in range(9):
    ax  = plt.subplot(3, 3, i+1)
    plt.imshow(images[i].numpy().astype("uint8"))

    predicted_class, confidence = predict(New_model, images[i].numpy())
    actual_class = class_names[labels[1]]

    plt.title(f"Actual: {actual_class},\n Predicted: {predicted_class}.\n Độ tin cậy: {confidence}%")

    plt.axis('off')

model_version = 1
New_model.save(f"/content/drive/MyDrive/Colab Notebooks/models{model_version}")